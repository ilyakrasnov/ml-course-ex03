{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read data from source\n",
    "file = 'data.xlsx'\n",
    "df = pd.read_excel(file, header=None)\n",
    "nr_rows = df.shape[0]\n",
    "nr_cols = df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Convert Y feature to number\n",
    "def numberize(value): \n",
    "    return 1 if value == 'M' else 0\n",
    "\n",
    "df[0] = df[0].apply(lambda x: numberize(x))\n",
    "\n",
    "    \n",
    "## Normalize data\n",
    "def norm(vector):\n",
    "    min = np.amin(vector)\n",
    "    max = np.amax(vector)\n",
    "    return (vector - min) / (min - max )\n",
    "\n",
    "for i in range(nr_cols):\n",
    "    df[:][i] = norm(df[:][i])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Split X and Y\n",
    "X = df.ix[:,1:]\n",
    "Y = df.ix[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.521037</td>\n",
       "      <td>-0.022658</td>\n",
       "      <td>-0.545989</td>\n",
       "      <td>-0.363733</td>\n",
       "      <td>-0.593753</td>\n",
       "      <td>-0.792037</td>\n",
       "      <td>-0.703140</td>\n",
       "      <td>-0.731113</td>\n",
       "      <td>-0.686364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.620776</td>\n",
       "      <td>-0.141525</td>\n",
       "      <td>-0.668310</td>\n",
       "      <td>-0.450698</td>\n",
       "      <td>-0.601136</td>\n",
       "      <td>-0.619292</td>\n",
       "      <td>-0.568610</td>\n",
       "      <td>-0.912027</td>\n",
       "      <td>-0.598462</td>\n",
       "      <td>-0.418864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.643144</td>\n",
       "      <td>-0.272574</td>\n",
       "      <td>-0.615783</td>\n",
       "      <td>-0.501591</td>\n",
       "      <td>-0.289880</td>\n",
       "      <td>-0.181768</td>\n",
       "      <td>-0.203608</td>\n",
       "      <td>-0.348757</td>\n",
       "      <td>-0.379798</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.606901</td>\n",
       "      <td>-0.303571</td>\n",
       "      <td>-0.539818</td>\n",
       "      <td>-0.435214</td>\n",
       "      <td>-0.347553</td>\n",
       "      <td>-0.154563</td>\n",
       "      <td>-0.192971</td>\n",
       "      <td>-0.639175</td>\n",
       "      <td>-0.233590</td>\n",
       "      <td>-0.222878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.601496</td>\n",
       "      <td>-0.390260</td>\n",
       "      <td>-0.595743</td>\n",
       "      <td>-0.449417</td>\n",
       "      <td>-0.514309</td>\n",
       "      <td>-0.431017</td>\n",
       "      <td>-0.462512</td>\n",
       "      <td>-0.635686</td>\n",
       "      <td>-0.509596</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.556386</td>\n",
       "      <td>-0.360075</td>\n",
       "      <td>-0.508442</td>\n",
       "      <td>-0.374508</td>\n",
       "      <td>-0.483590</td>\n",
       "      <td>-0.385375</td>\n",
       "      <td>-0.359744</td>\n",
       "      <td>-0.835052</td>\n",
       "      <td>-0.403706</td>\n",
       "      <td>-0.213433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.210090</td>\n",
       "      <td>-0.360839</td>\n",
       "      <td>-0.233501</td>\n",
       "      <td>-0.102906</td>\n",
       "      <td>-0.811321</td>\n",
       "      <td>-0.811361</td>\n",
       "      <td>-0.565604</td>\n",
       "      <td>-0.522863</td>\n",
       "      <td>-0.776263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.248310</td>\n",
       "      <td>-0.385928</td>\n",
       "      <td>-0.241347</td>\n",
       "      <td>-0.094008</td>\n",
       "      <td>-0.915472</td>\n",
       "      <td>-0.814012</td>\n",
       "      <td>-0.548642</td>\n",
       "      <td>-0.884880</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.773711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.629893</td>\n",
       "      <td>-0.156578</td>\n",
       "      <td>-0.630986</td>\n",
       "      <td>-0.489290</td>\n",
       "      <td>-0.430351</td>\n",
       "      <td>-0.347893</td>\n",
       "      <td>-0.463918</td>\n",
       "      <td>-0.518390</td>\n",
       "      <td>-0.378283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.519744</td>\n",
       "      <td>-0.123934</td>\n",
       "      <td>-0.506948</td>\n",
       "      <td>-0.341575</td>\n",
       "      <td>-0.437364</td>\n",
       "      <td>-0.172415</td>\n",
       "      <td>-0.319489</td>\n",
       "      <td>-0.558419</td>\n",
       "      <td>-0.157500</td>\n",
       "      <td>-0.142595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3         4         5         6         7   \\\n",
       "0   1 -0.521037 -0.022658 -0.545989 -0.363733 -0.593753 -0.792037 -0.703140   \n",
       "1   1 -0.643144 -0.272574 -0.615783 -0.501591 -0.289880 -0.181768 -0.203608   \n",
       "2   1 -0.601496 -0.390260 -0.595743 -0.449417 -0.514309 -0.431017 -0.462512   \n",
       "3   1 -0.210090 -0.360839 -0.233501 -0.102906 -0.811321 -0.811361 -0.565604   \n",
       "4   1 -0.629893 -0.156578 -0.630986 -0.489290 -0.430351 -0.347893 -0.463918   \n",
       "\n",
       "         8         9     ...           21        22        23        24  \\\n",
       "0 -0.731113 -0.686364    ...    -0.620776 -0.141525 -0.668310 -0.450698   \n",
       "1 -0.348757 -0.379798    ...    -0.606901 -0.303571 -0.539818 -0.435214   \n",
       "2 -0.635686 -0.509596    ...    -0.556386 -0.360075 -0.508442 -0.374508   \n",
       "3 -0.522863 -0.776263    ...    -0.248310 -0.385928 -0.241347 -0.094008   \n",
       "4 -0.518390 -0.378283    ...    -0.519744 -0.123934 -0.506948 -0.341575   \n",
       "\n",
       "         25        26        27        28        29        30  \n",
       "0 -0.601136 -0.619292 -0.568610 -0.912027 -0.598462 -0.418864  \n",
       "1 -0.347553 -0.154563 -0.192971 -0.639175 -0.233590 -0.222878  \n",
       "2 -0.483590 -0.385375 -0.359744 -0.835052 -0.403706 -0.213433  \n",
       "3 -0.915472 -0.814012 -0.548642 -0.884880 -1.000000 -0.773711  \n",
       "4 -0.437364 -0.172415 -0.319489 -0.558419 -0.157500 -0.142595  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Add '1' - Feature\n",
    "ones = np.ones(nr_rows, dtype=np.int)\n",
    "X.insert(0, '', ones)\n",
    "X.columns = range(df.shape[1])\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Prepare training set and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X.head(300)\n",
    "Y_train = Y.head(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nr_test_rows = nr_rows - 300\n",
    "X_test = X.tail(nr_test_rows)\n",
    "Y_test = Y.tail(nr_test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define helper methods\n",
    "\n",
    "def sig(z):\n",
    "    return 1 / (1 + math.e ** (-1*z))\n",
    "\n",
    "def gradient_descent(X, Y, alpha=0.1, max_iterations=100):\n",
    "    predictions = []\n",
    "    Theta = np.zeros(X.shape[1], dtype=np.int)  # Starting Theta\n",
    "    \n",
    "    while (max_iterations > 0):\n",
    "        prediction = sig(np.dot(X,Theta))\n",
    "        Theta = Theta + (alpha * (Y - prediction) * prediction * (1 - prediction)).dot(X)\n",
    "        predictions.append(prediction)\n",
    "        max_iterations -= 1\n",
    "    \n",
    "    return [predictions, Theta]\n",
    "\n",
    "# Transform p values to 1's and 0's\n",
    "crisp_predictor = lambda x: 1 if x >= 0.5 else 0 \n",
    "\n",
    "def accuracy(crisp, Y): # Subtract predictions from the real values and count the 1's\n",
    "    difference = pd.Series(crisp - Y)\n",
    "    correct_predictions = (difference == 0).sum()\n",
    "    total_predictions = crisp.shape[0]\n",
    "\n",
    "    return (correct_predictions / total_predictions * 100)\n",
    "\n",
    "def quality_test(Theta, X, Y):\n",
    "    predictions = sig(np.dot(X,Theta))\n",
    "    crisp = pd.Series(predictions).apply(crisp_predictor)\n",
    "    quality = accuracy(crisp, Y)\n",
    "    return quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run predictions by iterations of the algorithm\n",
    "def quality_by_iteration(alpha, max_iterations):\n",
    "    predictions_by_iteration = gradient_descent(X_train, Y_train, alpha, max_iterations)[0]\n",
    "    \n",
    "    \n",
    "    qualities = []\n",
    "    for prediction in predictions_by_iteration:\n",
    "        crisp = pd.Series(prediction).apply(crisp_predictor)\n",
    "        quality = accuracy(crisp, Y_train)\n",
    "        qualities.append(quality)\n",
    "\n",
    "    return qualities    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## MAIN Exercise = Select 5 best features\n",
    "alpha = 0.03\n",
    "max_iterations = 1000\n",
    "qualities = pd.Series(quality_by_iteration(alpha, max_iterations))  \n",
    "\n",
    "def test_accuracy(X_train, Y_train, X_test, Y_test):\n",
    "    final_theta = predictions_by_iteration = gradient_descent(X_train, Y_train, alpha, max_iterations)[1]\n",
    "    predictions = sig(np.dot(X_test,final_theta))\n",
    "    crisp = pd.Series(predictions).apply(crisp_predictor)\n",
    "    return accuracy(crisp, Y_test.values)\n",
    "    \n",
    "\n",
    "def find_best_features(X_train, Y_train, X_test, Y_test, max_features=5):\n",
    "    all_features = np.asarray(X_train.columns.values)\n",
    "    iterating_over = [0]\n",
    "    features_added = 0\n",
    "    for_comparison = { 'Nr of Features': [], 'Accuracy': []}\n",
    "    \n",
    "    while (features_added < max_features):\n",
    "        print(\"Adding feature nr \", features_added + 1)\n",
    "        accuracy_with_added_feature = {}    \n",
    "        for feature in all_features:\n",
    "            with_added_feature = list(iterating_over)\n",
    "            \n",
    "            # Add feature to dataset for iteration\n",
    "            if feature in with_added_feature:\n",
    "                # print(\"feature already in dataset\", feature)\n",
    "                continue\n",
    "            with_added_feature.append(feature) \n",
    "            new_X_train = X_train[with_added_feature]\n",
    "            new_X_test = X_test[with_added_feature]\n",
    "\n",
    "            calculated_accuracy = test_accuracy(new_X_train, Y_train, new_X_test, Y_test)\n",
    "            accuracy_with_added_feature[feature] = calculated_accuracy\n",
    "        \n",
    "        features_added += 1\n",
    "   \n",
    "        best_feature_to_add = max(accuracy_with_added_feature.items(), key=operator.itemgetter(1))[0]\n",
    "        #print(f\"Addiing feature # {best_feature_to_add} with added accuracy of {accuracy_with_added_feature[best_feature_to_add]}.\")\n",
    "        iterating_over.append(best_feature_to_add)\n",
    "        \n",
    "        for_comparison['Nr of Features'].append(features_added)\n",
    "        for_comparison['Accuracy'].append(accuracy_with_added_feature[best_feature_to_add])\n",
    "\n",
    "    print(f\"Finished finding {features_added} best features...\")    \n",
    "    return(for_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding feature nr  1\n",
      "Adding feature nr  2\n",
      "Adding feature nr  3\n",
      "Adding feature nr  4\n",
      "Adding feature nr  5\n",
      "Finished finding 5 best features...\n"
     ]
    }
   ],
   "source": [
    "accuracies_by_nr_of_features = find_best_features(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': [92.193308550185876,\n",
       "  95.167286245353154,\n",
       "  96.282527881040892,\n",
       "  96.6542750929368,\n",
       "  97.026022304832722,\n",
       "  97.77],\n",
       " 'Nr of Features': [1, 2, 3, 4, 5, 31]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_by_nr_of_features['Nr of Features'].append(31)\n",
    "accuracies_by_nr_of_features['Accuracy'].append(97.77)\n",
    "accuracies_by_nr_of_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(accuracies_by_nr_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11963d4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFdCAYAAADSax5EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyAAAGECQAQAwgCADAGAAQQYAwID/AyXoMOOpbkbIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119525a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = df.plot(x='Nr of Features', y='Accuracy', kind='bar')\n",
    "ax.set_ylim(90,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
